= ha-workshop

== Workshop Agenda

== Labs Overview

image::images/RHEL_HA_Networks.png[]

== Principles and Conventions of the Labs

*Code Entry*

Many of the labs will expect you to type commands into the CLI over a secure shell connection to a set of hosts provided. Each command entry will be highlighted, and where necessary the node we're expecting you to use will be identified. However, for clarity the conventions are identified below.

This is a command entry box:

....
# uname -a
Linux s01.fab.redhat.com 2.6.32-504.16.2.el6.x86_64 #1 SMP Tue Mar 10 17:01:00 EDT 2015 x86_64 x86_64 x86_64 GNU/Linux
....

Any commands that we expect you to run on a particular node will start with the node name and then a hash (nodea #), for example:

....
nodea # whoami
root
....

== RHEL HA Overview

=== Corosync

Corosync is the framework used by Pacemaker for handling communication between the cluster nodes. Corosync is also Pacemakerâ€™s source of membership and quorum data.

=== Pacemaker

This is the component responsible for all cluster-related activities, such as monitoring cluster membership, managing the services and resources, and fencing cluster members.

=== Fencing

Fencing is the disconnection of a node from the cluster's shared storage. Fencing cuts off I/O from shared storage, thus ensuring data integrity. The cluster infrastructure performs fencing through the STONITH facility. When Pacemaker determines that a node has failed, it communicates to other cluster-infrastructure components that the node has failed. STONITH fences the failed node when notified of the failure.

=== Quorum

In order to maintain cluster integrity and availability, cluster systems use a concept known as quorum to prevent data corruption and loss. A cluster has quorum when more than half of the cluster nodes are online. To mitigate the chance of data corruption due to failure, Pacemaker by default stops all resources if the cluster does not have quorum.

== Install and Configure

=== Create a basic cluster

We are going to create a 3 node cluster comprising of *nodea, nodeb and nodec*. Start by installing the RHEL HA packages on *each node*.

....
# yum install pcs pacemaker fence-agents-all
....

Enable cluster communications through the firewall on *each node*.

....
# firewall-cmd --permanent --add-service=high-availability
# firewall-cmd --add-service=high-availability
....

Set password for the pcs administration account on *all nodes*.

....
# echo Redhat123 | passwd --stdin hacluster
....

Start and enable the pcs daemon on *each node*.

....
# systemctl enable pcsd && systemctl start pcsd 
....

On *nodea*, authenticate the pcs admin user against each node in the cluster.

....
nodea # pcs cluster auth nodea-priv.example.com nodeb-priv.example.com nodec-priv.example.com
....

On *nodea*, create and start a cluster called cluster1 consisting of our 3 nodes.

....
nodea # pcs cluster setup --start --enable --name cluster1 nodea-priv.example.com nodeb-priv.example.com nodec-priv.example.com
....

Verify our cluster has been created succesfully.

....
nodea # pcs status
Cluster name: cluster1
WARNING: no stonith devices and stonith-enabled is not false
Stack: corosync
Current DC: nodea.example.com (version 1.1.18-11.el7_5.2-2b07d5c5a9) - partition with quorum
Last updated: Thu May 24 10:31:22 2018
Last change: Thu May 24 10:29:00 2018 by hacluster via crmd on nodea.example.com

3 nodes configured
0 resources configured

Online: [ nodea-priv.example.com nodeb-priv.example.com nodec-priv.example.com ]

No resources


Daemon Status:
  corosync: active/enabled
  pacemaker: active/enabled
  pcsd: active/enabled
....

Note the warning in the output telling us we haven't enabled fencing. We'll fix that next.

== Configure Fencing

We can list all of the available fencing agents.

....
nodea # pcs stonith list
....
Let's look at all of the available options for the IPMI fencing agent.

....
nodea # pcs stonith describe fence_ipmilan
fence_ipmilan - Fence agent for IPMI

fence_ipmilan is an I/O Fencing agentwhich can be used with machines controlled by IPMI.This agent calls support software ipmitool (http://ipmitool.sf.net/). WARNING! This fence agent might report success before the node is powered off. You should use -m/method onoff if your fence device works correctly with that option.

Stonith options:
  ipport: TCP/UDP port to use for connection with device
  hexadecimal_kg: Hexadecimal-encoded Kg key for IPMIv2 authentication
  port: IP address or hostname of fencing device (together with --port-as-ip)
  inet6_only: Forces agent to use IPv6 addresses only
  ipaddr: IP Address or Hostname
  passwd_script: Script to retrieve password
  method: Method to fence (onoff|cycle)
  inet4_only: Forces agent to use IPv4 addresses only
  passwd: Login password or passphrase
  lanplus: Use Lanplus to improve security of connection
  auth: IPMI Lan Auth type.
  cipher: Ciphersuite to use (same as ipmitool -C parameter)
  target: Bridge IPMI requests to the remote target address
  privlvl: Privilege level on IPMI device
  timeout: Timeout (sec) for IPMI operation
  login: Login Name
  verbose: Verbose mode
  debug: Write debug information to given file
  power_wait: Wait X seconds after issuing ON/OFF
  login_timeout: Wait X seconds for cmd prompt after login
  delay: Wait X seconds before fencing is started
  power_timeout: Test X seconds for status change after ON/OFF
  ipmitool_path: Path to ipmitool binary
  shell_timeout: Wait X seconds for cmd prompt after issuing command
  port_as_ip: Make "port/plug" to be an alias to IP address
  retry_on: Count of attempts to retry power on
  sudo: Use sudo (without password) when calling 3rd party sotfware.
  priority: The priority of the stonith resource. Devices are tried in order of highest priority to lowest.
  pcmk_host_map: A mapping of host names to ports numbers for devices that do not support host names. Eg. node1:1;node2:2,3 would tell the cluster to use port 1 for node1 and ports 2 and 3 for node2
  pcmk_host_list: A list of machines controlled by this device (Optional unless pcmk_host_check=static-list).
  pcmk_host_check: How to determine which machines are controlled by the device. Allowed values: dynamic-list (query the device), static-list (check the pcmk_host_list attribute), none (assume every device can
                   fence every machine)
  pcmk_delay_max: Enable a random delay for stonith actions and specify the maximum of random delay. This prevents double fencing when using slow devices such as sbd. Use this to enable a random delay for
                  stonith actions. The overall delay is derived from this random delay value adding a static delay so that the sum is kept below the maximum delay.
  pcmk_delay_base: Enable a base delay for stonith actions and specify base delay value. This prevents double fencing when different delays are configured on the nodes. Use this to enable a static delay for
                   stonith actions. The overall delay is derived from a random delay value adding this static delay so that the sum is kept below the maximum delay.
  pcmk_action_limit: The maximum number of actions can be performed in parallel on this device Pengine property concurrent-fencing=true needs to be configured first. Then use this to specify the maximum number
                     of actions can be performed in parallel on this device. -1 is unlimited.

Default operations:
  monitor: interval=60s
....

=== Libvirt Fencing

If you are using KVM virtualisation we will use the fence_xvm fencing agent. This agent talks back to the hypervisor to power machines on/off. First let's check that we can see all of the available VM's

....
nodea # fence_xvm -o list
nodea.example.com              e3d38597-e90c-4bfb-b1d2-144c4ef615b5 on
nodeb.example.com              d3b46128-6df0-4e9d-a7c6-d5bc260a9920 on
nodec.example.com              802a74f3-a533-4e76-8135-267b14a193e7 on
....

We can now create the fencing resources in pacemaker.

....
nodea # pcs stonith create nodea-fence fence_xvm pcmk_host_map="nodea-priv.example.com:nodea.example.com"
nodea # pcs stonith create nodeb-fence fence_xvm pcmk_host_map="nodeb-priv.example.com:nodeb.example.com"
nodea # pcs stonith create nodec-fence fence_xvm pcmk_host_map="nodec-priv.example.com:nodec.example.com"
....

We can confirm the fencing resources are working by running pcs status or pcs stonith.

....
nodea # pcs stonith
 nodea-fence	(stonith:fence_xvm):	Started nodea-priv.example.com
 nodeb-fence	(stonith:fence_xvm):	Started nodec-priv.example.com
 nodec-fence	(stonith:fence_xvm):	Started nodeb-priv.example.com
....

=== RHV-M Fencing

For RHV VM's we can use the fence_rhvm agent. This talks to the RHV Manager API to power cycle the cluster nodes. We can check that we can talk to the RHV API and see our vm's.

....
nodea # fence_rhevm -a rhv-m.example.com -l 'admin@internal' -p 'Redhat123' -z -o list --ssl-insecure --login-timeout=30 --disable-http-filter | grep node
nodeb.example.com,
nodea.example.com,
nodec.example.com,
....

....
nodea # pcs stonith create nodea-fence fence_rhevm ipaddr=rhvm.exmaple.com login='admin@internal' passwd='Redhat123' pcmk_host_map=nodea-priv.example.com:nodea.exmaple.com disable_http_filter=1 ssl_insecure=1 ssl=1
nodea # pcs stonith create nodeb-fence fence_rhevm ipaddr=rhvm.exmaple.com login='admin@internal' passwd='Redhat123' pcmk_host_map=nodeb-priv.example.com:nodeb.exmaple.com disable_http_filter=1 ssl_insecure=1 ssl=1
nodea # pcs stonith create nodec-fence fence_rhevm ipaddr=rhvm.exmaple.com login='admin@internal' passwd='Redhat123' pcmk_host_map=nodec-priv.example.com:nodec.exmaple.com disable_http_filter=1 ssl_insecure=1 ssl=1
....

We can confirm the fencing resources are working by running pcs status or pcs stonith.

....
nodea # pcs stonith
 nodea-fence	(stonith:fence_rhevm):	Started nodea-priv.example.com
 nodeb-fence	(stonith:fence_rhevm):	Started nodec-priv.example.com
 nodec-fence	(stonith:fence_rhevm):	Started nodeb-priv.example.com
....

=== Test Fencing

Finally, let's test the fencing agent.

....
nodea # pcs stonith fence nodeb-priv.example.com
....

Once the command prompt comes back we confirm the node has restarted. As these are virtual machines they restart quickly so we can follow the restart easily using watch.

....
nodea # watch pcs status 
Cluster name: cluster1
Stack: corosync
Current DC: nodea.example.com (version 1.1.18-11.el7_5.2-2b07d5c5a9) - partition with quorum
Last updated: Thu May 24 11:55:10 2018
Last change: Thu May 24 11:39:20 2018 by hacluster via crmd on nodeb.example.com

3 nodes configured
3 resources configured

Online: [ nodea-priv.example.com nodec-priv.example.com ]
OFFLINE: [ nodeb-priv.example.com ]

Full list of resources:

 nodea-fence	(stonith:fence_xvm):	Started nodea-priv.example.com
 nodeb-fence	(stonith:fence_xvm):	Started nodec-priv.example.com
 nodec-fence	(stonith:fence_xvm):	Started nodea-priv.example.com

Daemon Status:
  corosync: active/enabled
  pacemaker: active/enabled
  pcsd: active/enabled
....

The node should go OFFLINE before re-joining the cluster.

== Prepare Cluster Resources

We are going to configure a basic resource group which consists of the following.

. HA-LVM
. Filesystem
. Apache Web server
. Virtual IP

We need to prepare these resources before we can add them to the cluster.

=== Libvirt iSCSI configuration

*NOTE:* This is not required if we are deploying on RHV. First we need to discover the iSCSI targets and then login. We need to do this on *all nodes*.

....
# iscsiadm --mode discoverydb --type sendtargets --portal iscsi-storage.example.com --discover
# iscsiadm --mode node --targetname iqn.1994-05.com.redhat:iscsi-target --portal iscsi-storage.example.com --login
....

=== RHV Shared Storage

For RHV environments we can create a shared virtual disk and add this to each node.

=== Confirm storage is visible

dmesg should confirm we have discovered a disk - sda

....
[378055.438294] sd 2:0:0:0: [sda] 41934848 512-byte logical blocks: (21.4 GB/19.9 GiB)
[378055.438576] sd 2:0:0:0: [sda] Write Protect is off
[378055.438579] sd 2:0:0:0: [sda] Mode Sense: 43 00 10 08
[378055.438685] sd 2:0:0:0: [sda] Write cache: enabled, read cache: enabled, supports DPO and FUA
[378055.444792] sd 2:0:0:0: [sda] Attached SCSI disk
....

=== LVM and Filesystem

On *nodea* let's configure the LVM volume.

....
nodea # pvcreate /dev/sda
nodea # vgcreate ha_vg /dev/sda
nodea # lvcreate -L 5G -n ha_lv ha_vg
nodea # mkfs.xfs /dev/ha_vg/ha_lv
....

Now on *each node* we need to configure exclusive activation of a LVM volume group.

....
# lvmconf --enable-halvm --services --startstopservices
....

Next we need ensure the local volume groups will still be activated outside of the cluster. Edit /etc/lvm/lvm.conf on *each node* and add the following line.

./etc/lvm/lvm.conf
....
volume_list = [ "rhel" ]
....

Rebuild the ramdisk on *each node* to ensure the nodes will only activate their local volume group and not the cluster volume groups.

....
# dracut -H -f /boot/initramfs-$(uname -r).img $(uname -r)
# reboot
....

=== Apache

Now let's install apache on *all nodes* and enable http traffic through the firewall.

....
# yum -y install httpd php
# firewall-cmd --add-service=http --permanent
# firewall-cmd --add-service=http
....

== Create and Manage Cluster Resources

=== Creating Resources

We can list all available resource types:

....
# pcs resource list
....

To list the options for a particular resource:

....
# pcs resource describe LVM
Assumed agent name 'ocf:heartbeat:LVM' (deduced from 'LVM')
ocf:heartbeat:LVM - Controls the availability of an LVM Volume Group

Resource script for LVM. It manages an Linux Volume Manager volume (LVM) 
as an HA resource.

Resource options:
  volgrpname (required): The name of volume group.
  exclusive: If set, the volume group will be activated exclusively. This option works one of two ways. If the volume group has the cluster attribute
             set, then the volume group will be activated exclusively using clvmd across the cluster. If the cluster attribute is not set, the volume
             group will be activated exclusively through the use of the volume_list filter in lvm.conf. In the filter scenario, the LVM agent
             verifies that pacemaker's configuration will result in the volume group only being active on a single node in the cluster and that the
             local node's volume_list filter will prevent the volume group from activating outside of the resource agent. On activation this agent
             claims the volume group through the use of a unique tag, and then overrides the volume_list field in a way that allows the volume group
             to be activated only by the agent. To use exclusive activation without clvmd, the volume_list in lvm.conf must be initialized. If volume
             groups exist locally that are not controlled by the cluster, such as the root volume group, make sure those volume groups are listed in
             the volume_list so they will be allowed to activate on bootup.
  tag: If "exclusive" is set on a non clustered volume group, this overrides the tag to be used.
  partial_activation: If set, the volume group will be activated even only partial of the physical volumes available. It helps to set to true, when
                      you are using mirroring logical volumes.

Default operations:
  start: interval=0s timeout=30
  stop: interval=0s timeout=30
  monitor: interval=10 timeout=30
  methods: interval=0s timeout=5
....

Now we are going to create a resource group consisting of the following resources:

. HA-LVM
. Filesystem
. Apache Web server
. Virtual IP

Resources added to a resource group are started in the order they are added. First let's add the HA-LVM resource to activate the volume group.

....
nodea # pcs resource create www_vg LVM volgrpname=ha_vg exclusive=true --group webapp
....

Next we need to add a filesystem. We will mount our filesystem at /var/www/html which is our document root.

....
nodea # pcs resource create www_filesystem filesystem device=/dev/ha_vg/ha_lv directory=/var/www/html fstype=xfs --group webapp
....

The next resource we will add will be the virtual IP.

....
nodea # pcs resource create www_vip IPaddr2 ip=192.168.122.15 --group webapp
....

The final resource we need is apache.

....
nodea # pcs resource create www_apache systemd:httpd --group webapp
....

With the cluster resources online let's create a simple webpage in place which will show us the hostname we are running on. On the node running the resources create the file /var/www/html/index.php with the following contents.

./var/www/html/index.php
....
<!DOCTYPE html>
<html>
<body>

<?php
echo gethostname();
?>

</body>
</html>
....

Set SELinux contexts.

....
nodea # cp /root/index.php /var/www/html/index.php
nodea # restorecon -Rv /var/www/html/
....

We should now be able to access our website on it's virtual IP address.

=== Managing Resources

Let's quickly test if we can move our resource to a different node. First let's see what node it is running on.

....
# pcs status
Cluster name: cluster1
Stack: corosync
Current DC: nodea.example.com (version 1.1.18-11.el7_5.2-2b07d5c5a9) - partition with quorum
Last updated: Mon May 28 21:19:29 2018
Last change: Mon May 28 21:15:54 2018 by root via crm_resource on nodea.example.com

3 nodes configured
7 resources configured

Online: [ nodea-priv.example.com nodeb-priv.example.com nodec-priv.example.com ]

Full list of resources:

 nodea-fence	(stonith:fence_xvm):	Started nodea-priv.example.com
 nodeb-fence	(stonith:fence_xvm):	Started nodec-priv.example.com
 nodec-fence	(stonith:fence_xvm):	Started nodeb-priv.example.com
 Resource Group: webapp
     ha_vg	(ocf::heartbeat:LVM):	Started nodea-priv.example.com
     www_filesystem	(ocf::heartbeat:Filesystem):	Started nodea-priv.example.com
     www_vip	(ocf::heartbeat:IPaddr2):	Started nodea-priv.example.com
     www_apache	(ocf::heartbeat:apache):	Started nodea-priv.example.com

Daemon Status:
  corosync: active/enabled
  pacemaker: active/enabled
  pcsd: active/enabled
....

Now let's move the resource group. We will come back to the message about location constraints.

....
# pcs resource move webapp
Warning: Creating location constraint cli-ban-webapp-on-nodea.example.com with a score of -INFINITY for resource webapp on node nodea.example.com.
This will prevent webapp from running on nodea.example.com until the constraint is removed. This will be the case even if nodea.example.com is the last node in the cluster.
....

Now check if the resources are running on a different node.

So what was the message about constraints? When we move a resource it creates a constraint which prevents the resource group from moving back to that node. This constraint can be viewed and cleared as follows.

....
# pcs constraint
Location Constraints:
  Resource: webapp
    Disabled on: nodea.example.com (score:-INFINITY) (role: Started)
Ordering Constraints:
Colocation Constraints:
Ticket Constraints:
....

To clear the constraint we can run the following.

....
# pcs resource clear webapp
....

=== Stopping, starting & restarting Resources

The following commands stop, start and restart a resource.

....
# pcs resource disable www_apache
# pcs resource enable www_apache
# pcs resource restart www_apache
....

=== Unmanaging resources

Sometimes it can be useful to be able to stop and start resources outside of the clusters control. Unmanaging a resource stops pcs from actively manaing a resource. The below example would allow us to stop httpd without the cluster taking any action.

....
# pcs resource unmanage www_apache
# systemctl stop httpd
# pcs resource manage www_apache
....

=== Managing Nodes

Sometimes we need to perform maintenance on nodes and need to either stop them from running resources or particular resources. Placing a node in standby stops it from running any resources.

....
# pcs cluster standby nodea-priv.example.com
....

pcs status will confirm the node is now in standby. If it was running the cluster resources they will have moved to another node.

....
# pcs status
Cluster name: cluster1
Stack: corosync
Current DC: nodea.example.com (version 1.1.18-11.el7_5.2-2b07d5c5a9) - partition with quorum
Last updated: Thu May 31 15:11:14 2018
Last change: Thu May 31 15:11:05 2018 by root via cibadmin on nodea.example.com

3 nodes configured
7 resources configured

Node nodea-priv.example.com: standby
Online: [ nodeb-priv.example.com nodec-priv.example.com ]

Full list of resources:

 nodea-fence	(stonith:fence_xvm):	Started nodeb-priv.example.com
 nodeb-fence	(stonith:fence_xvm):	Started nodec-priv.example.com
 nodec-fence	(stonith:fence_xvm):	Started nodeb-priv.example.com
 Resource Group: webapp
     ha_vg	(ocf::heartbeat:LVM):	Started nodec-priv.example.com
     www_filesystem	(ocf::heartbeat:Filesystem):	Started nodec-priv.example.com
     www_vip	(ocf::heartbeat:IPaddr2):	Started nodec-priv.example.com
     www_app	(systemd:httpd):	Starting nodec-priv.example.com


Daemon Status:
  corosync: active/enabled
  pacemaker: active/enabled
  pcsd: active/enabled
....

To enable the node to run resources again we need to unstandby it.

....
# pcs cluster unstandby nodea-priv.example.com
....

Banning a resource just prevents that resource from running on that node. This may be useful if you have multiple resource groups and only want to prevent one of them from running on a node.

....
# pcs resource ban www_apache nodec-priv.example.com
# pcs resource clear www_apache
....

=== Node preference and resource stickiness

A preferred node can be set for resources to ensure it always runs on that particular node. Let's make nodea our preferred node by giving it a score of 200. This is how much we want it to run on that node. On it's own that number doesn't mean a lot but we'll come back to that later.

....
nodea # pcs constraint location webapp prefers nodea-priv.example.com=200
....

Now if we put nodea into standby the resource group will move to another cluster member. If we then unstandby it we should see the resource group move back to nodea.

....
nodea # pcs cluster standby nodea-priv.example.com
nodea # pcs cluster unstandby nodea-priv.example.com
....

The fact that the resource has moved back when it is in a perfectly healthy state may not be desireable. This causes unnecessary downtime for our application. This is where resource stickiness comes into play. This allows us to say how much we want our resource to stay where it currently is. If the score is higher than the score we gave to our preferred node then the resource should stay where it is.

Let's set our resource stickiness.

....
nodea # pcs resource defaults resource-stickiness=500
....

Now let's repeat our test. The resource group should stay where it is.

....
nodea # pcs cluster standby nodea-priv.example.com
nodea # pcs cluster unstandby nodea-priv.example.com
....

== Web UI

The pcs Web UI can be accessed on any of the nodes. For example, browse to https://nodea.example.com:2224 and login with the hacluster credentials we setup when we installed the cluster.


