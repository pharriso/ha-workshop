= ha-workshop

== Workshop Agenda

== Labs Overview

image::images/RHEL_HA_Networks.png[]

== Principles and Conventions of the Labs

*Code Entry*

Many of the labs will expect you to type commands into the CLI over a secure shell connection to a set of hosts provided. Each command entry will be highlighted, and where necessary the node we're expecting you to use will be identified. However, for clarity the conventions are identified below.

This is a command entry box:

....
# uname -a
Linux s01.fab.redhat.com 2.6.32-504.16.2.el6.x86_64 #1 SMP Tue Mar 10 17:01:00 EDT 2015 x86_64 x86_64 x86_64 GNU/Linux
....

Any commands that we expect you to run on a particular node will start with the node name and then a hash (nodea #), for example:

....
nodea # whoami
root
....

== RHEL HA Overview

=== Corosync

Corosync is the framework used by Pacemaker for handling communication between the cluster nodes. Corosync is also Pacemakerâ€™s source of membership and quorum data.

=== Pacemaker

This is the component responsible for all cluster-related activities, such as monitoring cluster membership, managing the services and resources, and fencing cluster members.

=== Fencing

Fencing is the disconnection of a node from the cluster's shared storage. Fencing cuts off I/O from shared storage, thus ensuring data integrity. The cluster infrastructure performs fencing through the STONITH facility. When Pacemaker determines that a node has failed, it communicates to other cluster-infrastructure components that the node has failed. STONITH fences the failed node when notified of the failure.

=== Quorum

In order to maintain cluster integrity and availability, cluster systems use a concept known as quorum to prevent data corruption and loss. A cluster has quorum when more than half of the cluster nodes are online. To mitigate the chance of data corruption due to failure, Pacemaker by default stops all resources if the cluster does not have quorum.

== Install and Configure

=== Create a basic cluster

We are going to create a 3 node cluster comprising of *nodea, nodeb and nodec*. Start by installing the RHEL HA packages on *each node*.

....
# yum install pcs pacemaker fence-agents-all
....

Enable cluster communications through the firewall on *each node*.

....
# firewall-cmd --permanent --add-service=high-availability
# firewall-cmd --add-service=high-availability
....

Set password for the pcs administration account on *all nodes*.

....
# echo Redhat123 | passwd --stdin hacluster
....

Start and enable the pcs daemon on *each node*.

....
# systemctl enable pcsd && systemctl start pcsd 
....

On *nodea*, authenticate the pcs admin user against each node in the cluster.

....
nodea # pcs cluster auth nodea-priv.example.com nodeb-priv.example.com nodec-priv.example.com
....

On *nodea*, create and start a cluster called cluster1 consisting of our 3 nodes.

....
nodea # pcs cluster setup --start --enable --name cluster1 nodea-priv.example.com nodeb-priv.example.com nodec-priv.example.com
....

Verify our cluster has been created succesfully.

....
nodea # pcs status
Cluster name: cluster1
WARNING: no stonith devices and stonith-enabled is not false
Stack: corosync
Current DC: nodea.example.com (version 1.1.18-11.el7_5.2-2b07d5c5a9) - partition with quorum
Last updated: Thu May 24 10:31:22 2018
Last change: Thu May 24 10:29:00 2018 by hacluster via crmd on nodea.example.com

3 nodes configured
0 resources configured

Online: [ nodea-priv.example.com nodeb-priv.example.com nodec-priv.example.com ]

No resources


Daemon Status:
  corosync: active/enabled
  pacemaker: active/enabled
  pcsd: active/enabled
....

Note the warning in the output telling us we haven't enabled fencing. We'll fix that next.

== Configure Fencing

We can list all of the available fencing agents.

....
nodea # pcs stonith list
....
Let's look at all of the available options for the IPMI fencing agent.

....
nodea # pcs stonith describe fence_ipmilan
fence_ipmilan - Fence agent for IPMI

fence_ipmilan is an I/O Fencing agentwhich can be used with machines controlled by IPMI.This agent calls support software ipmitool (http://ipmitool.sf.net/). WARNING! This fence agent might report success before the node is powered off. You should use -m/method onoff if your fence device works correctly with that option.

Stonith options:
  ipport: TCP/UDP port to use for connection with device
  hexadecimal_kg: Hexadecimal-encoded Kg key for IPMIv2 authentication
  port: IP address or hostname of fencing device (together with --port-as-ip)
  inet6_only: Forces agent to use IPv6 addresses only
  ipaddr: IP Address or Hostname
  passwd_script: Script to retrieve password
  method: Method to fence (onoff|cycle)
  inet4_only: Forces agent to use IPv4 addresses only
  passwd: Login password or passphrase
  lanplus: Use Lanplus to improve security of connection
  auth: IPMI Lan Auth type.
  cipher: Ciphersuite to use (same as ipmitool -C parameter)
  target: Bridge IPMI requests to the remote target address
  privlvl: Privilege level on IPMI device
  timeout: Timeout (sec) for IPMI operation
  login: Login Name
  verbose: Verbose mode
  debug: Write debug information to given file
  power_wait: Wait X seconds after issuing ON/OFF
  login_timeout: Wait X seconds for cmd prompt after login
  delay: Wait X seconds before fencing is started
  power_timeout: Test X seconds for status change after ON/OFF
  ipmitool_path: Path to ipmitool binary
  shell_timeout: Wait X seconds for cmd prompt after issuing command
  port_as_ip: Make "port/plug" to be an alias to IP address
  retry_on: Count of attempts to retry power on
  sudo: Use sudo (without password) when calling 3rd party sotfware.
  priority: The priority of the stonith resource. Devices are tried in order of highest priority to lowest.
  pcmk_host_map: A mapping of host names to ports numbers for devices that do not support host names. Eg. node1:1;node2:2,3 would tell the cluster to use port 1 for node1 and ports 2 and 3 for node2
  pcmk_host_list: A list of machines controlled by this device (Optional unless pcmk_host_check=static-list).
  pcmk_host_check: How to determine which machines are controlled by the device. Allowed values: dynamic-list (query the device), static-list (check the pcmk_host_list attribute), none (assume every device can
                   fence every machine)
  pcmk_delay_max: Enable a random delay for stonith actions and specify the maximum of random delay. This prevents double fencing when using slow devices such as sbd. Use this to enable a random delay for
                  stonith actions. The overall delay is derived from this random delay value adding a static delay so that the sum is kept below the maximum delay.
  pcmk_delay_base: Enable a base delay for stonith actions and specify base delay value. This prevents double fencing when different delays are configured on the nodes. Use this to enable a static delay for
                   stonith actions. The overall delay is derived from a random delay value adding this static delay so that the sum is kept below the maximum delay.
  pcmk_action_limit: The maximum number of actions can be performed in parallel on this device Pengine property concurrent-fencing=true needs to be configured first. Then use this to specify the maximum number
                     of actions can be performed in parallel on this device. -1 is unlimited.

Default operations:
  monitor: interval=60s
....

=== Libvirt Fencing

If you are using KVM virtualisation we will use the fence_xvm fencing agent. This agent talks back to the hypervisor to power machines on/off. First let's check that we can see all of the available VM's

....
nodea # fence_xvm -o list
nodea.example.com              e3d38597-e90c-4bfb-b1d2-144c4ef615b5 on
nodeb.example.com              d3b46128-6df0-4e9d-a7c6-d5bc260a9920 on
nodec.example.com              802a74f3-a533-4e76-8135-267b14a193e7 on
....

We can now create the fencing resources in pacemaker.

....
nodea # pcs stonith create nodea-fence fence_xvm pcmk_host_map="nodea-priv.example.com:nodea.example.com"
nodea # pcs stonith create nodeb-fence fence_xvm pcmk_host_map="nodeb-priv.example.com:nodeb.example.com"
nodea # pcs stonith create nodec-fence fence_xvm pcmk_host_map="nodec-priv.example.com:nodec.example.com"
....

We can confirm the fencing resources are working by running pcs status or pcs stonith.

....
nodea # pcs stonith
 nodea-fence	(stonith:fence_xvm):	Started nodea-priv.example.com
 nodeb-fence	(stonith:fence_xvm):	Started nodec-priv.example.com
 nodec-fence	(stonith:fence_xvm):	Started nodeb-priv.example.com
....

=== RHV-M Fencing

For RHV VM's we can use the fence_rhvm agent. This talks to the RHV Manager API to power cycle the cluster nodes. We can check that we can talk to the RHV API and see our vm's.

....
nodea # fence_rhevm -a rhv-m.example.com -l 'admin@internal' -p 'Redhat123' -z -o list --ssl-insecure --login-timeout=30 --disable-http-filter | grep node
nodeb.example.com,
nodea.example.com,
nodec.example.com,
....

....
nodea # pcs stonith create nodea-fence fence_rhevm ipaddr=rhvm.exmaple.com login='admin@internal' passwd='Redhat123' pcmk_host_map=nodea-priv.example.com:nodea.exmaple.com disable_http_filter=1 ssl_insecure=1 ssl=1
nodea # pcs stonith create nodeb-fence fence_rhevm ipaddr=rhvm.exmaple.com login='admin@internal' passwd='Redhat123' pcmk_host_map=nodeb-priv.example.com:nodeb.exmaple.com disable_http_filter=1 ssl_insecure=1 ssl=1
nodea # pcs stonith create nodec-fence fence_rhevm ipaddr=rhvm.exmaple.com login='admin@internal' passwd='Redhat123' pcmk_host_map=nodec-priv.example.com:nodec.exmaple.com disable_http_filter=1 ssl_insecure=1 ssl=1
....

We can confirm the fencing resources are working by running pcs status or pcs stonith.

....
nodea # pcs stonith
 nodea-fence	(stonith:fence_rhevm):	Started nodea-priv.example.com
 nodeb-fence	(stonith:fence_rhevm):	Started nodec-priv.example.com
 nodec-fence	(stonith:fence_rhevm):	Started nodeb-priv.example.com
....

=== Test Fencing

Finally, let's test the fencing agent.

....
nodea # pcs stonith fence nodeb-priv.example.com
....

Once the command prompt comes back we confirm the node has restarted. As these are virtual machines they restart quickly so we can follow the restart easily using watch.

....
nodea # watch pcs status 
Cluster name: cluster1
Stack: corosync
Current DC: nodea.example.com (version 1.1.18-11.el7_5.2-2b07d5c5a9) - partition with quorum
Last updated: Thu May 24 11:55:10 2018
Last change: Thu May 24 11:39:20 2018 by hacluster via crmd on nodeb.example.com

3 nodes configured
3 resources configured

Online: [ nodea-priv.example.com nodec-priv.example.com ]
OFFLINE: [ nodeb-priv.example.com ]

Full list of resources:

 nodea-fence	(stonith:fence_xvm):	Started nodea-priv.example.com
 nodeb-fence	(stonith:fence_xvm):	Started nodec-priv.example.com
 nodec-fence	(stonith:fence_xvm):	Started nodea-priv.example.com

Daemon Status:
  corosync: active/enabled
  pacemaker: active/enabled
  pcsd: active/enabled
....

The node should go OFFLINE before re-joining the cluster.

== Prepare Cluster Resources
